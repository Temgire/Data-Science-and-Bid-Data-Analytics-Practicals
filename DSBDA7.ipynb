{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb748f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mahi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mahi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mahi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mahi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb122ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunkssuch as words or sentences is called Tokenization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eba0c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunkssuch as words or sentences is called Tokenization.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text= sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077dbafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunkssuch', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba4e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'more', 'do', 'ours', 'couldn', 'about', 'such', 'same', 'before', 'its', 'now', 'whom', 'who', \"isn't\", 'her', 'and', 'himself', 'herself', 'any', \"it's\", 'for', 'she', \"needn't\", 'wasn', 'me', 'than', 'into', 'further', 'from', 'did', 'can', 'has', \"you're\", 'wouldn', \"wasn't\", 'mustn', \"she's\", 'during', 'does', 'above', \"that'll\", 'out', 's', 'my', 'once', \"you'll\", 'some', 'was', \"won't\", 'hers', 'until', 'again', 'will', 'off', 'other', \"weren't\", 'ain', 'here', 'weren', 'doing', 'theirs', 'of', \"haven't\", 'but', 'having', 'down', 'very', 'y', 'at', 'our', 'after', 'or', 've', 'them', 'because', 'between', 'over', 'not', 'ma', 'you', \"aren't\", \"hadn't\", 'll', 'there', \"doesn't\", 'yourselves', 'few', 'are', 'shan', 'shouldn', 'that', \"hasn't\", 'those', \"you've\", 'themselves', 'with', 'don', 'am', 'itself', 'didn', 'by', 'your', 'his', 'have', 'below', \"don't\", 'if', 'own', 'needn', 'is', 'most', \"didn't\", 'won', 'they', 'just', 'he', 'to', 'their', 'a', \"shouldn't\", 'yours', 'been', 'we', \"you'd\", 'the', 'no', 'only', 'o', 'doesn', 'him', 'up', 'isn', 'how', 'where', 'what', 'being', 'while', 'were', 'all', 'ourselves', 'an', \"mustn't\", 'it', 'be', 'each', 'in', 'so', 'hadn', 'under', 'which', \"wouldn't\", 'through', 'nor', 're', 'mightn', 'when', 'on', \"couldn't\", 'too', 'then', 'myself', 'm', 'had', 'aren', 'yourself', 'as', 'these', \"should've\", 'd', 'both', 'hasn', 'i', 'should', 'against', 'haven', 'this', \"mightn't\", \"shan't\", 'why', 't'}\n",
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'inpython', '?']\n",
      "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'inpython', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "text= \"How to remove stop words with NLTK library inPython?\"\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_text=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "          filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\",tokens)\n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8637dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\",\"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "print(rootWord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a35b80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "text = \"studies studying cries cry\" \n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64814b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('pink', 'NN')]\n",
      "[('sweater', 'NN')]\n",
      "[('fit', 'NN')]\n",
      "[('her', 'PRP$')]\n",
      "[('perfectly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"The pink sweater fit her perfectly\"\n",
    "words=word_tokenize(data)\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6031a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad7636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993d8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cbb5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "206280f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "    numOfWordsB = dict.fromkeys(uniqueWords,0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a971f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {} \n",
    "    bagOfWordsCount = len(bagOfWords) \n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5540abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b757d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "            idfDict[word] = math.log(N /float(val))\n",
    "            return idfDict\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a384b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsA,numOfWordsB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a5f6853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jupiter</th>\n",
       "      <th>Planet</th>\n",
       "      <th>from</th>\n",
       "      <th>planet</th>\n",
       "      <th>the</th>\n",
       "      <th>Sun</th>\n",
       "      <th>fourth</th>\n",
       "      <th>is</th>\n",
       "      <th>largest</th>\n",
       "      <th>Mars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Jupiter  Planet   from  planet  the    Sun  fourth    is  largest   Mars\n",
       "0  0.138629     0.2  0.000   0.000  0.4  0.000   0.000  0.40      0.2  0.000\n",
       "1  0.000000     0.0  0.125   0.125  0.5  0.125   0.125  0.25      0.0  0.125"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
